# 隐私保护与记忆隔离设计

## 📋 背景

在多用户 IRC 聊天环境中，当前所有用户的对话都存储在同一个 `conversation_history` 中。这带来了潜在的**记忆泄密风险**：

### 风险场景示例
```
用户 A: "我的 API Key 是 sk-abc123xyz"
用户 B: "刚才有人提到什么？"
Bot: "刚才用户 A 提到了他的 API Key..." ❌ 泄密！
```

## 🎯 设计目标

1. **防止跨用户信息泄露**：用户 A 的敏感信息不应被透露给用户 B
2. **保持对话连贯性**：Bot 仍需理解公共话题上下文
3. **实现成本可控**：不需要完整的用户级长期记忆系统
4. **性能友好**：不显著增加内存和计算开销

## 🔧 三种备选方案

### 方案 1：敏感信息过滤（推荐实现）⭐⭐⭐

**核心思路**：在消息进入历史记录前，自动检测和脱敏敏感信息。

#### 实现要点
```python
def is_sensitive_message(message: str) -> bool:
    """检测消息是否包含敏感信息"""
    sensitive_patterns = [
        r'密码[:：]\s*\S+',
        r'password[:：]\s*\S+',
        r'api[_\s]?key[:：]\s*\S+',
        r'token[:：]\s*\S+',
        r'secret[:：]\s*\S+',
        r'身份证\D*\d{15,18}',
        r'手机号[:：]?\s*1[3-9]\d{9}',
        r'银行卡',
        r'信用卡',
    ]
    for pattern in sensitive_patterns:
        if re.search(pattern, message, re.IGNORECASE):
            return True
    return False

def sanitize_message(message: str) -> str:
    """脱敏处理"""
    if is_sensitive_message(message):
        logger.warning(f"检测到敏感信息，已自动忽略: {message[:20]}...")
        return "[此消息包含敏感信息，已自动忽略]"
    return message
```

#### 优点
- ✅ 实现简单（只需添加一个正则过滤器）
- ✅ 立竿见影（阻止 90% 的常见泄密场景）
- ✅ 性能开销极小
- ✅ 不破坏现有架构

#### 缺点
- ❌ 无法识别所有敏感信息（如隐晦表达）
- ❌ 正则规则需要持续维护

#### 适用场景
- IRC 公开频道（本项目当前场景）
- 用户基本具备隐私意识
- 需要快速实现基础保护

---

### 方案 2：用户上下文隔离（中等复杂度）⭐⭐

**核心思路**：为每个用户维护独立的短期对话窗口，回复时优先参考该用户的历史。

#### 数据结构设计
```python
class AIAgent:
    def __init__(self, ...):
        # 全局历史（供理解公共话题）
        self.conversation_history: list[dict] = [...]
        
        # 用户独立上下文（最多保留最近 5 条）
        self.user_contexts: dict[str, list[dict]] = {}
        
        # 用户最后活跃时间
        self.user_last_seen: dict[str, datetime] = {}
```

#### 消息添加逻辑
```python
def add_message(self, sender: str, message: str):
    """添加消息时区分全局和用户上下文"""
    # 1. 敏感信息过滤
    safe_message = sanitize_message(message)
    
    # 2. 加入全局历史（供理解话题）
    self.conversation_history.append({
        "role": "user",
        "content": f"[来自 {sender}]: {safe_message}"
    })
    
    # 3. 加入用户独立上下文
    if sender not in self.user_contexts:
        self.user_contexts[sender] = []
    
    self.user_contexts[sender].append({
        "role": "user",
        "content": safe_message
    })
    
    # 4. 限制用户上下文大小（最多 5 条）
    if len(self.user_contexts[sender]) > 5:
        self.user_contexts[sender].pop(0)
    
    # 5. 更新活跃时间
    self.user_last_seen[sender] = datetime.now()
```

#### 响应生成逻辑
```python
def generate_response(self, sender: str, message: str):
    """生成回复时优先参考该用户的上下文"""
    # 构建混合上下文
    context_messages = []
    
    # 1. 系统提示
    context_messages.append(self.conversation_history[0])
    
    # 2. 全局历史（最近 10 条，供理解话题）
    context_messages.extend(self.conversation_history[-10:])
    
    # 3. 该用户的独立上下文（优先级更高）
    if sender in self.user_contexts:
        context_messages.append({
            "role": "system",
            "content": f"[{sender} 的历史对话上下文]"
        })
        context_messages.extend(self.user_contexts[sender])
    
    # 4. 当前消息
    context_messages.append({
        "role": "user",
        "content": f"[来自 {sender}]: {message}"
    })
    
    # 调用 LLM 生成回复
    response = self.client.chat.completions.create(
        model=self.openai_config.model,
        messages=context_messages,
        ...
    )
```

#### 优点
- ✅ 用户上下文隔离（A 的话题不会干扰 B）
- ✅ 保持公共话题理解能力
- ✅ 自动过期清理（超过 1 小时的用户上下文自动删除）

#### 缺点
- ❌ 实现复杂度中等
- ❌ 内存开销增加（每个用户 5 条消息）
- ❌ 仍需配合敏感信息过滤

#### 适用场景
- 需要更精细的用户对话管理
- 用户数量可控（< 100 人）
- 对隐私保护要求较高

---

### 方案 3：纯公开讨论模式（AI 约束）⭐

**核心思路**：在系统提示中约束 AI 的行为，依赖 LLM 的理解能力。

#### 系统提示增强
```python
system_prompt = """
你是 IRC 聊天室的参与者...

⚠️ 隐私保护规则：
1. 这是公开频道，所有对话都是公开的
2. 如果有人分享密码、API Key、身份证等敏感信息，立即提醒："⚠️ 请不要在公开频道分享敏感信息"
3. 绝不在回复中引用、重复或暗示他人的敏感信息
4. 只讨论技术、观点等适合公开的话题
5. 如果被问及他人的私密信息，回答："这是他人的隐私，我不便讨论"
"""
```

#### 优点
- ✅ 实现成本最低（只需修改提示词）
- ✅ 不改变代码架构
- ✅ 利用 LLM 的理解能力

#### 缺点
- ❌ 依赖 LLM 的"自觉性"（不可靠）
- ❌ 提示词注入攻击风险
- ❌ 无法保证 100% 有效

#### 适用场景
- 作为其他方案的补充手段
- 快速原型验证
- 用户群体可信任

---

## 📊 方案对比表

| 维度 | 方案 1：敏感信息过滤 | 方案 2：用户上下文隔离 | 方案 3：AI 约束 |
|------|-------------------|-------------------|---------------|
| **实现复杂度** | ⭐ 低 | ⭐⭐ 中 | ⭐ 最低 |
| **隐私保护效果** | 🛡️ 90% | 🛡️ 95% | 🛡️ 60% |
| **性能开销** | 📊 极小 | 📊 中等 | 📊 无 |
| **维护成本** | 🔧 低 | 🔧 中 | 🔧 极低 |
| **是否需要用户级记忆** | ❌ 否 | ⚠️ 简化版 | ❌ 否 |

---

## 🎯 推荐实施方案

### 阶段 1：立即实施（方案 1 + 方案 3）
1. 实现敏感信息过滤器（正则匹配）
2. 增强系统提示（AI 约束）
3. 添加用户警告机制

**预期效果**：阻止 90% 的常见泄密场景，实现成本 < 2 小时。

### 阶段 2：按需实施（方案 2）
如果未来出现以下情况，再考虑实施用户上下文隔离：
- 用户投诉隐私泄露问题
- 频道从公开转为半私密
- 需要支持用户级个性化功能

---

## 🔍 实施检查清单

- [ ] 添加 `is_sensitive_message()` 函数
- [ ] 添加 `sanitize_message()` 函数
- [ ] 在 `add_message()` 中集成过滤器
- [ ] 更新系统提示（三个 Agent 的 config）
- [ ] 添加单元测试（`test_privacy_filter.py`）
- [ ] 添加用户警告机制
- [ ] 更新 README.md（添加隐私保护说明）
- [ ] 创建 CHANGELOG（如 `CHANGELOG_PRIVACY_PROTECTION.md`）

---

## 📚 参考资料

### 常见敏感信息模式
```python
SENSITIVE_PATTERNS = {
    "密码类": [
        r'密码[:：]\s*\S+',
        r'password[:：]\s*\S+',
        r'passwd[:：]\s*\S+',
    ],
    "密钥类": [
        r'api[_\s]?key[:：]\s*\S+',
        r'secret[_\s]?key[:：]\s*\S+',
        r'token[:：]\s*\S+',
        r'access[_\s]?token[:：]\s*\S+',
    ],
    "证件类": [
        r'身份证\D*\d{15,18}',
        r'护照号',
        r'驾驶证',
    ],
    "金融类": [
        r'银行卡\D*\d{16,19}',
        r'信用卡',
        r'支付宝账号',
        r'微信号[:：]\s*\S+',
    ],
    "联系方式": [
        r'手机号[:：]?\s*1[3-9]\d{9}',
        r'电话[:：]?\s*\d{3,4}[-\s]?\d{7,8}',
        r'邮箱[:：]?\s*\S+@\S+\.\S+',
    ],
}
```

### IRC 隐私最佳实践
1. 明确告知用户"这是公开频道"
2. 机器人定期发送隐私提醒
3. 提供私密讨论的替代渠道（私信、DM）
4. 日志脱敏后再持久化

---

## 🚀 未来展望

如果需要完整的"用户级长期记忆"系统，考虑以下技术栈：
- **向量数据库**：Chroma、Pinecone、Qdrant（语义检索）
- **结构化存储**：为每个用户维护独立的 SQLite/PostgreSQL 表
- **记忆摘要**：定期压缩旧对话为摘要（GPT-4 Turbo）
- **权限控制**：用户可查看/删除自己的记忆
- **加密存储**：敏感信息加密后存储

但基于当前项目需求（实验性 IRC 聊天），**方案 1 + 方案 3 已足够**。

---

## 📝 版本历史

- **2025-10-13**：初始文档（lemonhall）
  - 定义三种隐私保护方案
  - 推荐实施路径
  - 预留扩展空间

---

## 💬 讨论记录

**Q**: 为什么不直接实现完整的用户级记忆？  
**A**: 工程量大（需要数据库、向量检索等），而当前是公开 IRC 频道，用户本身就应该有隐私意识。方案 1（敏感信息过滤）+ 方案 3（AI 约束）已能覆盖 90% 的风险场景，性价比最高。

**Q**: 如果用户故意绕过过滤器怎么办？  
**A**: 
1. 正则规则持续迭代（添加更多模式）
2. 考虑使用 LLM 辅助检测（调用 API 判断是否敏感）
3. 频道管理员人工介入

**Q**: 方案 2 会不会导致 Bot 无法理解跨用户对话？  
**A**: 不会。方案 2 是"混合上下文"设计：
- 全局历史（最近 10 条）→ 理解公共话题
- 用户独立上下文（最近 5 条）→ 理解用户个人话题
- LLM 会综合两者生成回复

---

**文档维护者**: lemonhall  
**最后更新**: 2025-10-13
