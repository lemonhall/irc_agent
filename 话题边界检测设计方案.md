# IRC Agent 话题边界检测（自动分段记忆）设计方案

## 🎯 功能目标

话题边界检测是向量记忆系统的重要组件，旨在解决以下核心问题：

1. **话题混淆**：长时间对话中多个话题交织，AI 难以准确理解当前讨论重点
2. **记忆污染**：旧话题的信息干扰新话题的理解和回应
3. **上下文过载**：超长对话历史导致重要信息被稀释
4. **检索精度**：无结构化的对话历史难以进行精确的语义检索

### 预期效果

```
场景示例：
[话题1: 数据库选型] → [话题边界] → [话题2: 前端框架讨论] → [话题边界] → [话题3: 部署方案]

优化前：所有消息混在一起，AI 可能在讨论部署时还在纠结数据库问题
优化后：每个话题独立管理，AI 能专注当前讨论，必要时精确召回相关历史
```

## 🧠 核心设计理念

### 1. 多层次检测机制

话题边界检测不能依赖单一指标，需要多个维度综合判断：

- **语义相似度**：基于向量嵌入计算消息间的语义距离
- **关键词分析**：识别明确的话题转换信号词
- **时间间隔**：利用现有的时间感知机制
- **对话结构**：分析问答、总结、结论等对话模式
- **用户意图**：检测用户主动切换话题的信号

### 2. 渐进式分段策略

不是每条消息都要判断，而是在特定条件下触发检测：

```python
触发条件优先级：
1. 用户主动分段命令（!newtopic, !topic, 换个话题等）
2. 长时间沉默后的首条消息（基于现有60分钟重置机制）
3. 语义突变检测（与最近N条消息的相似度显著下降）
4. 话题结束信号（如"好的，那就这样"、"明白了"、"总结一下"）
5. 定期检查点（每10条消息检查一次）
```

## 🏗️ 技术架构设计

### 数据结构重构

#### 当前结构
```python
# ai_agent.py 当前实现
conversation_history: list[dict] = [
    {"role": "system", "content": "..."},
    {"role": "user", "content": "[来自 user1 在 #channel]: 消息内容"},
    {"role": "assistant", "content": "回复内容"},
    # ...更多消息
]
```

#### 重构后结构
```python
# 新的分段记忆结构
class TopicSegment:
    """话题片段"""
    def __init__(self):
        self.id: str = ""                    # 片段唯一ID
        self.start_time: datetime = None     # 开始时间
        self.end_time: datetime = None       # 结束时间
        self.participants: set[str] = set()  # 参与用户
        self.messages: list[dict] = []       # 消息列表
        self.summary: str = ""               # AI生成的话题摘要
        self.keywords: list[str] = []        # 关键词标签
        self.embedding: list[float] = []     # 话题向量表示
        self.importance_score: float = 0.0   # 重要性评分（0-10）

class SegmentedMemory:
    """分段记忆管理器"""
    def __init__(self):
        self.segments: list[TopicSegment] = []
        self.current_segment: TopicSegment = None
        self.max_segments_in_memory = 5      # 内存中保留的片段数
        self.segment_id_counter = 0
        
        # 边界检测配置
        self.similarity_threshold = 0.7      # 语义相似度阈值
        self.min_segment_length = 3          # 最小片段长度（消息数）
        self.max_segment_length = 50         # 最大片段长度
        self.boundary_check_interval = 10    # 检查间隔（消息数）
```

### 边界检测算法

#### 1. 语义相似度检测
```python
def calculate_semantic_similarity(self, new_message: str, recent_messages: list[str]) -> float:
    """
    计算新消息与最近消息的语义相似度
    
    使用策略：
    1. 对新消息和最近3-5条消息生成embedding
    2. 计算余弦相似度
    3. 取平均值作为整体相似度
    """
    # 使用OpenAI的text-embedding-3-small模型
    # 成本低，速度快，适合实时计算
    pass

def detect_semantic_boundary(self, new_message: str) -> bool:
    """
    基于语义相似度判断是否为话题边界
    
    判断逻辑：
    - 相似度 < 0.3：强烈信号，很可能是新话题
    - 相似度 0.3-0.7：中等信号，结合其他因素判断
    - 相似度 > 0.7：弱信号，继续当前话题
    """
    pass
```

#### 2. 关键词信号检测
```python
def detect_keyword_signals(self, message: str) -> dict:
    """
    检测话题转换的关键词信号
    
    返回：
    {
        "topic_start": bool,      # 新话题开始信号
        "topic_end": bool,        # 当前话题结束信号
        "explicit_command": bool,  # 明确的分段命令
        "confidence": float       # 置信度 0-1
    }
    """
    
    # 话题开始信号
    topic_start_patterns = [
        r"换个话题",
        r"说点别的",
        r"另外",
        r"对了",
        r"关于.*?",
        r"我想问.*?",
        r"顺便问.*?",
        r"by the way",
        r"anyway",
    ]
    
    # 话题结束信号
    topic_end_patterns = [
        r"好的[，。]?就这样",
        r"明白了",
        r"总结一下",
        r"就这个问题",
        r"差不多了",
        r"ok[，。]?那",
        r"行[，。]?那",
    ]
    
    # 明确分段命令
    explicit_commands = [
        r"!newtopic",
        r"!topic",
        r"!分段",
        r"!新话题",
    ]
```

#### 3. 对话结构分析
```python
def analyze_conversation_structure(self, recent_messages: list[dict]) -> dict:
    """
    分析对话结构，识别自然的话题边界
    
    检测模式：
    1. 问答完成模式：问题→回答→确认
    2. 讨论总结模式：讨论→总结→新方向
    3. 决策结束模式：选项→决定→执行计划
    """
    
    patterns = {
        "question_answered": False,   # 问题已被回答
        "discussion_summarized": False,  # 讨论已被总结
        "decision_made": False,       # 决策已做出
        "confirmation_received": False  # 已收到确认
    }
    
    # 使用轻量级NLP分析对话模式
    # 可以考虑使用gpt-4o-mini进行结构化分析
```

#### 4. 综合决策引擎
```python
def should_create_boundary(self, new_message: str, context: dict) -> tuple[bool, float, str]:
    """
    综合决策是否创建话题边界
    
    返回: (是否分段, 置信度, 原因说明)
    """
    
    factors = {
        "semantic_similarity": 0.0,    # 语义相似度得分
        "keyword_signals": 0.0,        # 关键词信号得分
        "time_gap": 0.0,              # 时间间隔得分
        "structure_analysis": 0.0,     # 结构分析得分
        "segment_length": 0.0,         # 当前片段长度得分
    }
    
    # 权重配置（可调）
    weights = {
        "semantic_similarity": 0.4,
        "keyword_signals": 0.3,
        "time_gap": 0.1,
        "structure_analysis": 0.15,
        "segment_length": 0.05,
    }
    
    # 计算综合得分
    total_score = sum(factors[k] * weights[k] for k in factors)
    
    # 决策阈值
    if total_score > 0.8:
        return True, total_score, "强烈信号：多个指标支持分段"
    elif total_score > 0.6:
        return True, total_score, "中等信号：建议分段"
    else:
        return False, total_score, "继续当前话题"
```

### 话题摘要生成

每当创建新的话题边界时，为刚结束的片段生成摘要：

```python
def generate_segment_summary(self, segment: TopicSegment) -> str:
    """
    为话题片段生成摘要
    
    摘要包含：
    1. 主要讨论内容（1-2句）
    2. 关键决策或结论
    3. 重要信息点
    4. 参与者观点差异（如有）
    """
    
    prompt = f"""
    请为以下对话片段生成简洁摘要（不超过100字）：
    
    参与者：{', '.join(segment.participants)}
    时间：{segment.start_time} - {segment.end_time}
    消息数：{len(segment.messages)}
    
    对话内容：
    {self._format_messages_for_summary(segment.messages)}
    
    要求：
    1. 概括主要讨论话题
    2. 记录重要决策或结论
    3. 标注关键信息点
    4. 保持客观中性
    """
    
    # 使用gpt-4o-mini生成摘要（成本控制）
```

## 📊 性能优化策略

### 1. 嵌入计算优化
```python
# 批量计算embedding，减少API调用
def batch_compute_embeddings(self, texts: list[str]) -> list[list[float]]:
    """批量计算文本嵌入，最多100条/次"""
    
# 缓存机制
class EmbeddingCache:
    """嵌入向量缓存，避免重复计算"""
    def __init__(self, max_size=1000):
        self.cache = {}
        self.max_size = max_size
    
    def get_or_compute(self, text: str) -> list[float]:
        text_hash = hashlib.md5(text.encode()).hexdigest()
        if text_hash in self.cache:
            return self.cache[text_hash]
        
        # 计算新的embedding并缓存
        embedding = self._compute_embedding(text)
        if len(self.cache) >= self.max_size:
            # LRU清理
            oldest_key = next(iter(self.cache))
            del self.cache[oldest_key]
        
        self.cache[text_hash] = embedding
        return embedding
```

### 2. 检测频率控制
```python
def should_check_boundary(self, message_count: int, last_check: int) -> bool:
    """
    控制边界检测频率，避免过度计算
    
    策略：
    1. 用户主动命令：立即检测
    2. 关键词信号：立即检测
    3. 定期检查：每10条消息
    4. 时间触发：长时间沉默后
    """
    
    # 避免过于频繁的语义相似度计算
    return (
        message_count - last_check >= 10 or  # 定期检查
        self._has_explicit_signals(message) or  # 明确信号
        self._significant_time_gap(message)     # 时间间隔
    )
```

## 🔧 集成实现方案

### 阶段1：基础框架搭建（Week 1-2）
```python
# 1. 在ai_agent.py中添加分段记忆类
# 2. 重构现有对话历史结构
# 3. 实现基本的话题片段管理
# 4. 添加简单的关键词检测

# 关键文件修改：
# - ai_agent.py: 添加SegmentedMemory类
# - 新增topic_detector.py: 话题检测逻辑
# - test_topic_detection.py: 单元测试
```

### 阶段2：语义检测集成（Week 3-4）
```python
# 1. 集成OpenAI embedding API
# 2. 实现语义相似度计算
# 3. 优化检测算法和阈值
# 4. 添加性能监控

# 新增依赖：
# uv add numpy scikit-learn  # 向量计算
```

### 阶段3：摘要与存储（Week 5-6）
```python
# 1. 实现话题摘要生成
# 2. 添加片段持久化存储
# 3. 集成向量数据库（ChromaDB）
# 4. 实现历史片段检索

# 新增依赖：
# uv add chromadb  # 向量数据库
```

### 阶段4：优化与测试（Week 7-8）
```python
# 1. 性能优化和缓存机制
# 2. 大量真实对话测试
# 3. 参数调优和阈值校准
# 4. 用户体验优化
```

## 🧪 测试策略

### 单元测试
```python
# test_topic_detection.py

def test_keyword_detection():
    """测试关键词检测准确性"""
    
def test_semantic_similarity():
    """测试语义相似度计算"""
    
def test_boundary_decision():
    """测试综合决策逻辑"""
    
def test_segment_management():
    """测试片段管理功能"""
```

### 集成测试
```python
# test_segmented_conversation.py

def test_real_conversation_flow():
    """使用真实对话数据测试完整流程"""
    
def test_multi_topic_discussion():
    """测试多话题讨论的分段效果"""
    
def test_performance_with_long_history():
    """测试长对话历史的性能表现"""
```

### 用户验收测试
```python
# 准备测试场景：
scenarios = [
    "技术讨论 → 生活闲聊",
    "项目规划 → 具体实现 → 测试策略",
    "问题诊断 → 解决方案 → 总结回顾",
    "多人讨论中的话题分叉",
]
```

## 📈 效果评估指标

### 定量指标
1. **分段准确率**：人工标注vs系统自动分段的一致性
2. **召回精度**：基于话题检索相关历史片段的准确性
3. **响应延迟**：边界检测对系统响应速度的影响
4. **内存使用**：分段存储vs传统列表的内存效率

### 定性指标
1. **对话连贯性**：AI回复是否更贴近当前话题
2. **记忆准确性**：跨时间召回历史信息的准确性
3. **用户体验**：用户感知的对话质量提升
4. **系统稳定性**：长时间运行的稳定性表现

## 🔮 未来扩展方向

### 短期优化（3个月内）
- [ ] 支持多语言话题检测
- [ ] 添加情感分析维度
- [ ] 实现用户个性化阈值
- [ ] 优化移动端性能

### 中期发展（6个月内）
- [ ] 多模态话题检测（图片、链接）
- [ ] 跨频道话题关联
- [ ] 话题趋势分析
- [ ] 智能话题推荐

### 长期愿景（1年内）
- [ ] 基于图神经网络的话题关系建模
- [ ] 实时话题热度追踪
- [ ] 群体智慧挖掘
- [ ] 知识图谱构建

---

**文档版本**：v1.0  
**创建时间**：2025-10-13  
**作者**：GitHub Copilot  
**关联文档**：向量记忆系统设计方案.md、隐私保护与记忆隔离设计.md